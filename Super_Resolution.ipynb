{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Super-Resultion.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "etoWJCqqBBJq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Section"
      ]
    },
    {
      "metadata": {
        "id": "BuXTXaiWBBQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Path\n",
        "import os\n",
        "\n",
        "# Data\n",
        "import numpy as np\n",
        "\n",
        "# Image\n",
        "from PIL import Image # used in image processing function\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# For Tensorflow Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "\n",
        "# Google Drive\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R2Th2ldi5Pi7",
        "colab_type": "code",
        "outputId": "5eb700dc-1b75-414b-8e6e-2af7a7d1c433",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'tensorflow' from '/usr/local/lib/python3.6/dist-packages/tensorflow/__init__.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "pAr52j3pAKOZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab Function"
      ]
    },
    {
      "metadata": {
        "id": "7BI277Ug8aiC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gpu_check():\n",
        "  \"\"\"\n",
        "      Check GPU RAM status. Since google colab share gpu resource amount of its\n",
        "      user, you want to make sure there are enough GPU RAM that are free to use.\n",
        "      Recommend at least 3000MB free GPU RAM.\n",
        "  \"\"\"\n",
        "  # memory footprint support libraries/code\n",
        "  !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "  !pip install gputil\n",
        "  !pip install psutil\n",
        "  !pip install humanize\n",
        "  import psutil\n",
        "  import humanize\n",
        "  import os\n",
        "  import GPUtil as GPU\n",
        "  GPUs = GPU.getGPUs()\n",
        "  # XXX: only one GPU on Colab and isn’t guaranteed\n",
        "  gpu = GPUs[0]\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4He-bP3u-087",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def mount_drive():\n",
        "  \"\"\"\n",
        "      Mount your google drive to google colab, so that you can access the\n",
        "      dataset via google drive.\n",
        "      \n",
        "      YOUR DIR PATH SHOULD BE: gdrive/\"Colab Notebooks\"\n",
        "  \"\"\"\n",
        "  drive.mount('/content/gdrive')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cy0RU340NMu7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Config"
      ]
    },
    {
      "metadata": {
        "id": "aC2LlAquNz8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    config project in this cell.\n",
        "\"\"\"\n",
        "\n",
        "# Env, where you running this project\n",
        "# Google colab: 'colab'\n",
        "env = 'colab'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-RshcPfONLU2",
        "colab_type": "code",
        "outputId": "35a776b6-2266-4e82-84be-48efce3f310e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "     _______________________________________________\n",
        "    | YOU SHOULD NOT CHANGE ANY THING IN THIS CELL. |\n",
        "    |_______________________________________________|\n",
        "\"\"\"\n",
        "\n",
        "# config dictionary\n",
        "config = {\n",
        "    'colab':{\n",
        "        'data_dir':'/content/gdrive/My Drive/Colab Notebooks/data',\n",
        "        'checkpoint_dir':'/content/gdrive/My Drive/Colab Notebooks/checkpoints',\n",
        "        'log_dir':'/content/gdrive/My Drive/Colab Notebooks/logs',\n",
        "        'weight_dir':'/content/gdrive/My Drive/Colab Notebooks/weights'\n",
        "    }\n",
        "}\n",
        "\n",
        "# setting all variables\n",
        "data_dir = config[env]['data_dir']\n",
        "checkpoint_dir = config[env]['checkpoint_dir']\n",
        "log_dir = config[env]['log_dir']\n",
        "weight_dir = config[env]['weight_dir']\n",
        "\n",
        "if env == 'colab':\n",
        "  # mount google dirve\n",
        "  mount_drive()\n",
        "  # check GPU\n",
        "  gpu_check()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.8 GB  | Proc size: 305.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kDu_20FxAARs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Processing Function"
      ]
    },
    {
      "metadata": {
        "id": "c_98TDEO_pzJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def array_to_img(x, mode='YCbCr'):\n",
        "  \"\"\"\n",
        "      Convert array to image using YCbCr color\n",
        "      \n",
        "      Args:\n",
        "        x: array of image.\n",
        "        mode: channel mode, default to 'YCbCr'\n",
        "              1 (1-bit pixels, black and white, stored with one pixel per byte)\n",
        "              L (8-bit pixels, black and white)\n",
        "              P (8-bit pixels, mapped to any other mode using a color palette)\n",
        "              RGB (3x8-bit pixels, true color)\n",
        "              RGBA (4x8-bit pixels, true color with transparency mask)\n",
        "              CMYK (4x8-bit pixels, color separation)\n",
        "              YCbCr (3x8-bit pixels, color video format)\n",
        "              Note that this refers to the JPEG, and not the ITU-R BT.2020, standard\n",
        "              LAB (3x8-bit pixels, the L*a*b color space)\n",
        "              HSV (3x8-bit pixels, Hue, Saturation, Value color space)\n",
        "              I (32-bit signed integer pixels)\n",
        "              F (32-bit floating point pixels)\n",
        "  \"\"\"\n",
        "  return Image.fromarray(x.astype('uint8'), mode=mode)\n",
        "\n",
        "\n",
        "def bicubic_rescale(image, scale):\n",
        "  \"\"\"\n",
        "      Rescale image using bicubic interpolation.\n",
        "      \n",
        "      Args:\n",
        "        image: image\n",
        "        scale: use integer for up scaling. use 1/integer for down scaling\n",
        "  \"\"\"\n",
        "  # make sure scale is valid\n",
        "  if isinstance(scale, (float, int)):\n",
        "    size = (np.array(image.size) * scale).astype(int)\n",
        "  '''\n",
        "  WARNING\n",
        "  image.resize might lead to image displacement\n",
        "  https://hackernoon.com/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35\n",
        "  switch to tf.image.resize_bicubic\n",
        "  '''\n",
        "  return image.resize(size, resample=Image.BICUBIC)\n",
        "\n",
        "\n",
        "def modcrop(image, scale):\n",
        "  \"\"\"\n",
        "      To scale down the original image, there must be no remainder while scaling\n",
        "      operation.\n",
        "      \n",
        "      All we want to do in here is to subtract the remainder from height and \n",
        "      width of original image size, and cut the original image to the new size.\n",
        "      \n",
        "      Args:\n",
        "        image: original image\n",
        "        scale: must be int\n",
        "  \"\"\"\n",
        "  if not isinstance(scale, int):\n",
        "    raise Exception('utils.modcrop: scale must be int')\n",
        "  size = np.array(image.size)\n",
        "  size -= size % scale\n",
        "  return image.crop([0, 0, *size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kQdMEWBzeZUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "Q2pYcMrkefv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image_pair(path, scale=3, greyscale=False):\n",
        "  \"\"\"\n",
        "      Down scaling a hight resolution image to a low resolution image and\n",
        "      return both of them.\n",
        "      \n",
        "      Args:\n",
        "        path: image path\n",
        "        scale: scale of down scaling, must be a int\n",
        "        greyscale: return only Y channel\n",
        "  \"\"\"\n",
        "  image = load_img(path)\n",
        "  image = image.convert('YCbCr')\n",
        "  \n",
        "  if greyscale:\n",
        "    Y, Cb, Cr = image.split()\n",
        "    Y.show()\n",
        "    image = Y\n",
        "  \n",
        "  hr_image = modcrop(image, scale)\n",
        "  lr_image = bicubic_rescale(hr_image, 1 / scale)\n",
        "  return lr_image, hr_image\n",
        "\n",
        "\n",
        "def generate_sub_images(image, size, stride):\n",
        "  \"\"\"\n",
        "      Cut image into sub images.\n",
        "      \n",
        "      Args:\n",
        "        image: image\n",
        "        size: size of sub image\n",
        "        stride: distance of how much the window shifts by in each of the \n",
        "                dimensions\n",
        "  \"\"\"\n",
        "  for i in range(0, image.size[0] - size + 1, stride):\n",
        "        for j in range(0, image.size[1] - size + 1, stride):\n",
        "            # yield return a generator, or a list of number\n",
        "            yield image.crop([i, j, i + size, j + size])\n",
        "\n",
        "\n",
        "def load_set(dataset_name, lr_sub_size, lr_sub_stride, scale, greyscale=False):\n",
        "  \"\"\"\n",
        "      Load all image from a directory and cut them into small sub image.\n",
        "      \n",
        "      Args:\n",
        "        dataset_name: name of dir of the data set\n",
        "        lr_sub_size: low resolution sub image size\n",
        "        lr_sub_stride: stride when crop sub image\n",
        "        scale: down scale value\n",
        "        greyscale: return only Y channel\n",
        "  \"\"\"\n",
        "  if not all(isinstance(i, int) for i in [lr_sub_size, lr_sub_stride, scale]):\n",
        "    raise Exception('utils.load_set: lr_sub_size, stride, scale must be int')\n",
        "    \n",
        "  # compute parameters for hight resolution image\n",
        "  hr_sub_size = lr_sub_size * scale\n",
        "  hr_sub_stride = lr_sub_stride * scale\n",
        "\n",
        "  lr_sub_arrays = []\n",
        "  hr_sub_arrays = []\n",
        "  for file_name in os.listdir(os.path.join(data_dir, dataset_name)):\n",
        "    path = os.path.join(data_dir, dataset_name, file_name)\n",
        "    lr_image, hr_image = load_image_pair(str(path), scale=scale, greyscale=greyscale)\n",
        "    lr_sub_arrays += [img_to_array(img) for img in generate_sub_images(lr_image, size=lr_sub_size, stride=lr_sub_stride)]\n",
        "    hr_sub_arrays += [img_to_array(img) for img in generate_sub_images(hr_image, size=hr_sub_size, stride=hr_sub_stride)]\n",
        "  \n",
        "  # convert list to np.array\n",
        "  x = np.stack(lr_sub_arrays)\n",
        "  y = np.stack(hr_sub_arrays)\n",
        "  \n",
        "  return x, y\n",
        "\n",
        "\n",
        "# TODO\n",
        "# normalization?\n",
        "# Seng: add a layer called BatchNormalization in model\n",
        "# https://keras.io/layers/normalization/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YdOAzDpH50m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper Funcion"
      ]
    },
    {
      "metadata": {
        "id": "6ra2c266H5U5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model,\n",
        "    train_set,\n",
        "    val_set,\n",
        "    epochs=1,\n",
        "    steps_per_epoch=30,\n",
        "    validation_steps=3,\n",
        "    resume=True):\n",
        "  \"\"\"\n",
        "    train function for all model.\n",
        "    \n",
        "    \n",
        "  \"\"\"\n",
        "  # define callbacks\n",
        "  callbacks = [\n",
        "    # Save checkpoints of model at regular intervals\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_dir,\n",
        "        save_best_only=True\n",
        "    ),\n",
        "      \n",
        "    # Interrupt training if `val_loss` stops improving for over 2 epochs\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=2,\n",
        "        monitor='val_loss'\n",
        "    ),\n",
        "      \n",
        "    # Write TensorBoard logs to `./logs` directory\n",
        "    tf.keras.callbacks.TensorBoard(\n",
        "        log_dir=log_dir\n",
        "    )\n",
        "  ]\n",
        "  \n",
        "  # inherit weights\n",
        "  if resume:\n",
        "    # get model name\n",
        "    name = ''\n",
        "    if isinstance(model, type(srcnn)):\n",
        "      name = 'srcnn'\n",
        "      \n",
        "    model.load_weights(os.path.join(weight_dir,name))\n",
        "  \n",
        "  # Train\n",
        "  model.fit(train_set, epochs=epochs, steps_per_epoch=steps_per_epoch,\n",
        "            validation_data=val_set, validation_steps=validation_steps)\n",
        "  \n",
        "  # TODO: plot metrics\n",
        "  \n",
        "\n",
        "def test(model, test_set, steps=30, metrics=None):\n",
        "  # test\n",
        "  model.evaluate(test_set, steps=steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Njz9qRTbR2eC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "aopmTkWXR5Yt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def srcnn(img_size, f1=9, n1=64, n2=32, f3=5):\n",
        "  '''\n",
        "  input_image: the sub-image of label y, better in 32\n",
        "  f1: filter size(must be odd #), f1 x f1 \n",
        "  n1: the number of filter apply on layer 1.\n",
        "  n2: the number of filter apply on layer 2.\n",
        "  f3: filter size(must be odd #), f3 x f3 \n",
        "  \n",
        "  \n",
        "  from: https://arxiv.org/abs/1501.00092\n",
        "  '''  \n",
        "  \n",
        "  if not isinstance(img_size, (int)):\n",
        "    raise Exception('img_size is not a valid size in srcnn model')\n",
        "  \n",
        "  model = Sequential()\n",
        "  initializer = RandomNormal(mean=0.0, stddev=0.001)\n",
        "  model.add(Conv2D(filters=n1, kernel_size=f1, padding='same', bias_initializer='zeros', kernel_initializer=initializer,\n",
        "                   use_bias=True,activation='relu', input_shape=(img_size,img_size,1)))\n",
        "  model.add(Conv2D(filters=n2, kernel_size=1, padding='same', kernel_initializer=initializer,\n",
        "                   use_bias=True,activation='relu',bias_initializer='zeros'))\n",
        "  mode.add(Conv2D(filters=1, #filter here need to match the channel\n",
        "                   kernel_size=f3, padding='same', bias_initializer='zeros', kernel_initializer=initializer,\n",
        "                   use_bias=True,activation='relu'))\n",
        "  \n",
        "  # either SGD or Adam, paper used SGD, but Adam used widely\n",
        "  optimizer = SGD(lr=0.0003)\n",
        "  #optimizer = Adam(lr=0.0003)\n",
        "  model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  \n",
        "  model.summary()\n",
        "  \n",
        "  return model\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W5lUP8blWp7_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def srcnn_with_normalization(img_size, f1=9, n1=64, n2=32, f3=5):\n",
        "  '''\n",
        "  input_image: the sub-image of label y, better in 32\n",
        "  f1: filter size(must be odd #), f1 x f1 \n",
        "  n1: the number of filter apply on layer 1.\n",
        "  n2: the number of filter apply on layer 2.\n",
        "  f3: filter size(must be odd #), f3 x f3 \n",
        "  \n",
        "  \n",
        "  from: https://arxiv.org/abs/1501.00092\n",
        "  '''  \n",
        "  \n",
        "  if not isinstance(img_size, (int)):\n",
        "    raise Exception('img_size is not a valid size in srcnn_with_normalization model')\n",
        "  \n",
        "  model = Sequential()\n",
        "  initializer = RandomNormal(mean=0.0, stddev=0.001)\n",
        "  # First Layer\n",
        "  model.add(Conv2D(filters=n1, kernel_size=f1, padding='same', input_shape=(img_size,img_size,1),kernel_initializer=initializer))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  # Second Layer\n",
        "  model.add(Conv2D(filters=n2, kernel_size=1, padding='same', kernel_initializer=initializer))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  # Third Layer\n",
        "  mode.add(Conv2D(filters=1,kernel_size=f3, padding='same', kernel_initializer=initializer))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Activation('relu'))\n",
        "  \n",
        "  # either SGD or Adam, paper used SGD, but Adam used widely\n",
        "  optimizer = SGD(lr=0.0003)\n",
        "  #optimizer = Adam(lr=0.0003)\n",
        "  model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  \n",
        "  model.summary()\n",
        "  \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2cgYjtF81t1s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ]
    },
    {
      "metadata": {
        "id": "SGkYBCzi2AWC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Config"
      ]
    },
    {
      "metadata": {
        "id": "h9UOu_Bm1uPg",
        "colab_type": "code",
        "outputId": "559c5233-19a4-4d06-b4e0-4e35bd9a6c81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        }
      },
      "cell_type": "code",
      "source": [
        "# size of sub image\n",
        "size = 33\n",
        "\n",
        "# strde when crop image\n",
        "stride = 14\n",
        "\n",
        "# upscaling factor\n",
        "scale = 3\n",
        "\n",
        "# batch size\n",
        "batch = 32\n",
        "\n",
        "# greyscale ON or OFF? if ON, img will only contain Y channel\n",
        "greyscale = True\n",
        "\n",
        "# whcih training dataset you want to use?\n",
        "train_dataset_dir = '91-image'\n",
        "\n",
        "# which validation dataset you want to use?\n",
        "val_dataset_dir = 'Set5'\n",
        "\n",
        "# which testing dataset you want to use?\n",
        "test_dataset_dir = 'Set14'\n",
        "\n",
        "# which model you want to train or test?\n",
        "model = srcnn(size, 9, 64, 32, 5)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2KU8_I55mV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make Data"
      ]
    },
    {
      "metadata": {
        "id": "PXgxe40I4OWB",
        "colab_type": "code",
        "outputId": "62da4523-d376-4441-c350-085f6886acb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "     _______________________________________________\n",
        "    | YOU SHOULD NOT CHANGE ANY THING IN THIS CELL. |\n",
        "    |_______________________________________________|\n",
        "\"\"\"\n",
        "\n",
        "# load tranning dataset\n",
        "train_lr, train_hr = load_set(train_dataset_dir, size, stride, scale, greyscale)\n",
        "\n",
        "# TODO: 放大图片啊！！！！！！！！！！！！！！！！！！！！！！！！！！！！！！\n",
        "# blur a sub-image by a proper Gaussian kernel\n",
        "\n",
        "# make tf.data dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_lr, train_hr))\n",
        "train_dataset = train_dataset.batch(batch)\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "# load validation dataset\n",
        "val_lr, val_hr = load_set(val_dataset_dir, size, stride, scale, greyscale)\n",
        "# make tf.data dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_lr, val_hr))\n",
        "val_dataset = val_dataset.batch(batch//10)\n",
        "val_dataset = val_dataset.repeat()\n",
        "print(train_dataset)\n",
        "# Train\n",
        "# train(model, train_dataset, val_dataset, resume=False)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((?, 33, 33, 1), (?, 99, 99, 1)), types: (tf.float32, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZNHswUv_B4J1",
        "colab_type": "code",
        "outputId": "2b419de4-8f35-4f0d-c388-70274d9ecfde",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "train_lr.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1478, 33, 33, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "GJvRMpeyAz9Z",
        "colab_type": "code",
        "outputId": "e18b2911-de90-44bd-98d3-b3dc144f9b98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        }
      },
      "cell_type": "code",
      "source": [
        "train(model, train_dataset, val_dataset)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-236a4f1080ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-bcc86acc9ff1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_set, val_set, epochs, steps_per_epoch, validation_steps, resume)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'srcnn'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m   \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1483\u001b[0;31m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1484\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m    324\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_api_names_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train.NewCheckpointReader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on /content/gdrive/My Drive/Colab Notebooks/weights/: Not found: /content/gdrive/My Drive/Colab Notebooks/weights; No such file or directory"
          ]
        }
      ]
    }
  ]
}