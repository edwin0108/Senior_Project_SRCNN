{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spuer_Resolution.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "etoWJCqqBBJq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Import Section"
      ]
    },
    {
      "metadata": {
        "id": "ZSlATWxYTLjs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BuXTXaiWBBQK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Path\n",
        "import os\n",
        "\n",
        "# Data\n",
        "import numpy as np\n",
        "\n",
        "# Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Image\n",
        "from PIL import Image # used in image processing function\n",
        "from PIL import ImageFilter # used in load_image_pair()\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "\n",
        "# For Tensorflow Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "# Google Drive\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pAr52j3pAKOZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Colab Function"
      ]
    },
    {
      "metadata": {
        "id": "7BI277Ug8aiC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gpu_check():\n",
        "  \"\"\"\n",
        "      Check GPU RAM status. Since google colab share gpu resource amount of its\n",
        "      user, you want to make sure there are enough GPU RAM that are free to use.\n",
        "      Recommend at least 3000MB free GPU RAM.\n",
        "  \"\"\"\n",
        "  # memory footprint support libraries/code\n",
        "  !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "  !pip install gputil\n",
        "  !pip install psutil\n",
        "  !pip install humanize\n",
        "  import psutil\n",
        "  import humanize\n",
        "  import os\n",
        "  import GPUtil as GPU\n",
        "  GPUs = GPU.getGPUs()\n",
        "  # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "  gpu = GPUs[0]\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jCYudY3lCm9Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Path Functions"
      ]
    },
    {
      "metadata": {
        "id": "m8cFuFGVCmUi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ensure_dir(path_to_dir):\n",
        "  try:\n",
        "      os.makedirs(path_to_dir)\n",
        "  except FileExistsError:\n",
        "      # directory already exists\n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cy0RU340NMu7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Config"
      ]
    },
    {
      "metadata": {
        "id": "aC2LlAquNz8_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "    config project in this cell.\n",
        "\"\"\"\n",
        "\n",
        "# Env, where you running this project\n",
        "# Google colab: 'colab'\n",
        "env = 'colab'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-RshcPfONLU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "8e77ee04-b42b-4d11-8120-b0d05e791a51"
      },
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "     _______________________________________________\n",
        "    | YOU SHOULD NOT CHANGE ANY THING IN THIS CELL. |\n",
        "    |_______________________________________________|\n",
        "\"\"\"\n",
        "\n",
        "# config dictionary\n",
        "config = {\n",
        "    'colab':{\n",
        "        'data_dir':'/content/drive/My Drive/Colab Notebooks/data',\n",
        "        'checkpoint_dir':'/content/drive/My Drive/Colab Notebooks/checkpoints',\n",
        "        'log_dir':'/content/drive/My Drive/Colab Notebooks/logs',\n",
        "    }\n",
        "}\n",
        "\n",
        "# setting all variables\n",
        "data_dir = config[env]['data_dir']\n",
        "checkpoint_dir = config[env]['checkpoint_dir']\n",
        "log_dir = config[env]['log_dir']\n",
        "\n",
        "# mount google drive\n",
        "if env == 'colab':\n",
        "  # mount google dirve\n",
        "  drive.mount('/content/drive')\n",
        "  # ensure dir exsits\n",
        "  ensure_dir(checkpoint_dir)\n",
        "  # check GPU\n",
        "  gpu_check()\n",
        "  "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.4 GB  | Proc size: 2.7 GB\n",
            "GPU RAM Free: 11101MB | Used: 340MB | Util   3% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kDu_20FxAARs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Image Processing Function"
      ]
    },
    {
      "metadata": {
        "id": "c_98TDEO_pzJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def array_to_img(x, mode='YCbCr'):\n",
        "  \"\"\"\n",
        "      Convert array to image using YCbCr color\n",
        "      \n",
        "      Args:\n",
        "        x: array of image.\n",
        "        mode: channel mode, default to 'YCbCr'\n",
        "              1 (1-bit pixels, black and white, stored with one pixel per byte)\n",
        "              L (8-bit pixels, black and white)\n",
        "              P (8-bit pixels, mapped to any other mode using a color palette)\n",
        "              RGB (3x8-bit pixels, true color)\n",
        "              RGBA (4x8-bit pixels, true color with transparency mask)\n",
        "              CMYK (4x8-bit pixels, color separation)\n",
        "              YCbCr (3x8-bit pixels, color video format)\n",
        "              Note that this refers to the JPEG, and not the ITU-R BT.2020, standard\n",
        "              LAB (3x8-bit pixels, the L*a*b color space)\n",
        "              HSV (3x8-bit pixels, Hue, Saturation, Value color space)\n",
        "              I (32-bit signed integer pixels)\n",
        "              F (32-bit floating point pixels)\n",
        "  \"\"\"\n",
        "  return Image.fromarray(x.astype('uint8'), mode=mode)\n",
        "\n",
        "\n",
        "def bicubic_rescale(image, scale):\n",
        "  \"\"\"\n",
        "      Rescale image using bicubic interpolation.\n",
        "      \n",
        "      Args:\n",
        "        image: image\n",
        "        scale: use integer for up scaling. use 1/integer for down scaling\n",
        "  \"\"\"\n",
        "  # make sure scale is valid\n",
        "  if isinstance(scale, (float, int)):\n",
        "    size = (np.array(image.size) * scale).astype(int)\n",
        "  '''\n",
        "  WARNING\n",
        "  image.resize might lead to image displacement\n",
        "  https://hackernoon.com/how-tensorflows-tf-image-resize-stole-60-days-of-my-life-aba5eb093f35\n",
        "  switch to tf.image.resize_bicubic\n",
        "  '''\n",
        "  return image.resize(size, resample=Image.BICUBIC)\n",
        "\n",
        "\n",
        "def modcrop(image, scale):\n",
        "  \"\"\"\n",
        "      To scale down the original image, there must be no remainder while scaling\n",
        "      operation.\n",
        "      \n",
        "      All we want to do in here is to subtract the remainder from height and \n",
        "      width of original image size, and cut the original image to the new size.\n",
        "      \n",
        "      Args:\n",
        "        image: original image\n",
        "        scale: must be int\n",
        "  \"\"\"\n",
        "  if not isinstance(scale, int):\n",
        "    raise Exception('utils.modcrop: scale must be int')\n",
        "  size = np.array(image.size)\n",
        "  size -= size % scale\n",
        "  return image.crop([0, 0, *size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kQdMEWBzeZUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "Q2pYcMrkefv7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_image_pair(path, scale=3, greyscale=False, same_size=False):\n",
        "  \"\"\"\n",
        "      Down scaling a hight resolution image to a low resolution image and\n",
        "      return both of them.\n",
        "      \n",
        "      Args:\n",
        "        path: image path\n",
        "        scale: scale of down scaling, must be a int\n",
        "        greyscale: return only Y channel\n",
        "  \"\"\"\n",
        "  image = load_img(path)\n",
        "  image = image.convert('YCbCr')\n",
        "  \n",
        "  if greyscale:\n",
        "    Y, Cb, Cr = image.split()\n",
        "    Y.show()\n",
        "    image = Y\n",
        "  \n",
        "  hr_image = modcrop(image, scale)\n",
        "  lr_image = bicubic_rescale(hr_image, 1 / scale)\n",
        "  lr_image = bicubic_rescale(lr_image, scale)\n",
        "  lr_image = lr_image.filter(ImageFilter.GaussianBlur(radius=2))\n",
        "  return lr_image, hr_image\n",
        "\n",
        "\n",
        "def generate_sub_images(image, size, stride):\n",
        "  \"\"\"\n",
        "      Cut image into sub images.\n",
        "      \n",
        "      Args:\n",
        "        image: image\n",
        "        size: size of sub image\n",
        "        stride: distance of how much the window shifts by in each of the \n",
        "                dimensions\n",
        "  \"\"\"\n",
        "  for i in range(0, image.size[0] - size + 1, stride):\n",
        "        for j in range(0, image.size[1] - size + 1, stride):\n",
        "            # yield return a generator, or a list of number\n",
        "            yield image.crop([i, j, i + size, j + size])\n",
        "\n",
        "\n",
        "def load_set(dataset_name, lr_sub_size, lr_sub_stride, scale, same_size=False, greyscale=False):\n",
        "  \"\"\"\n",
        "      Load all image from a directory and cut them into small sub image.\n",
        "      \n",
        "      Args:\n",
        "        dataset_name: name of dir of the data set\n",
        "        lr_sub_size: low resolution sub image size\n",
        "        lr_sub_stride: stride when crop sub image\n",
        "        scale: down scale value\n",
        "        same_size: hr, lr have the same size\n",
        "        greyscale: return only Y channel\n",
        "  \"\"\"\n",
        "  if not all(isinstance(i, int) for i in [lr_sub_size, lr_sub_stride, scale]):\n",
        "    raise Exception('utils.load_set: lr_sub_size, stride, scale must be int')\n",
        "    \n",
        "  # compute parameters for hight resolution image\n",
        "  if same_size:\n",
        "    hr_sub_size = lr_sub_size\n",
        "    hr_sub_stride = lr_sub_stride\n",
        "  else:\n",
        "    hr_sub_size = lr_sub_size * scale\n",
        "    hr_sub_stride = lr_sub_stride * scale\n",
        "\n",
        "  lr_sub_arrays = []\n",
        "  hr_sub_arrays = []\n",
        "  for file_name in os.listdir(os.path.join(data_dir, dataset_name)):\n",
        "    path = os.path.join(data_dir, dataset_name, file_name)\n",
        "    lr_image, hr_image = load_image_pair(str(path), scale=scale, greyscale=greyscale, same_size=same_size)\n",
        "    lr_sub_arrays += [img_to_array(img) for img in generate_sub_images(lr_image, size=lr_sub_size, stride=lr_sub_stride)]\n",
        "    hr_sub_arrays += [img_to_array(img) for img in generate_sub_images(hr_image, size=hr_sub_size, stride=hr_sub_stride)]\n",
        "  \n",
        "  # convert list to np.array\n",
        "  x = np.stack(lr_sub_arrays)\n",
        "  y = np.stack(hr_sub_arrays)\n",
        "  \n",
        "  return x, y\n",
        "\n",
        "\n",
        "# TODO\n",
        "# normalization?\n",
        "# Seng: add a layer called BatchNormalization in model\n",
        "# https://keras.io/layers/normalization/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YdOAzDpH50m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Helper Function"
      ]
    },
    {
      "metadata": {
        "id": "6ra2c266H5U5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(\n",
        "    model,\n",
        "    model_name,\n",
        "    train_set,\n",
        "    val_set,\n",
        "    epochs=1,\n",
        "    steps_per_epoch=30,\n",
        "    validation_steps=3,\n",
        "    resume=True):\n",
        "  \"\"\"\n",
        "    train function for all model.\n",
        "    \n",
        "    \n",
        "  \"\"\"\n",
        "  path_to_weight_file = os.path.join(checkpoint_dir,model_name)\n",
        "\n",
        "  # define callbacks\n",
        "  callbacks = [\n",
        "    # Save checkpoints of model at regular intervals\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=path_to_weight_file,\n",
        "        save_best_only=True\n",
        "    )\n",
        "  ]\n",
        "  \n",
        "  # inherit weights\n",
        "  if resume:\n",
        "    model.load_weights(path_to_weight_file)\n",
        "  \n",
        "  # Train\n",
        "  history = model.fit(train_set, epochs=epochs, callbacks=callbacks, steps_per_epoch=steps_per_epoch,\n",
        "            validation_data=val_set, validation_steps=validation_steps)\n",
        "  \n",
        "  # plot metrics\n",
        "  plot_based_accuracy(history)\n",
        "  plot_based_loss(history)\n",
        "  \n",
        "\n",
        "def test(model, test_set, steps=30, metrics=None):\n",
        "  # test\n",
        "  model.evaluate(test_set, steps=steps)\n",
        "\n",
        "def plot_based_loss(history):\n",
        "  '''\n",
        "  Plot training & validation loss values\n",
        "  history: a dictionary recording training loss values and metrics values at successive epochs\n",
        "  '''\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "  \n",
        "def plot_based_accuracy(history):\n",
        "  '''\n",
        "  Plot training & validation accuracy values\n",
        "  history: a dictionary recording training loss values and metrics values at successive epochs\n",
        "  '''\n",
        "  plt.plot(history.history['acc'])\n",
        "  plt.plot(history.history['val_acc'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Njz9qRTbR2eC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model"
      ]
    },
    {
      "metadata": {
        "id": "aopmTkWXR5Yt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def srcnn(img_size, channel=1, f1=9, n1=64, n2=32, f3=5):\n",
        "  '''\n",
        "  img_size: the size of the input image\n",
        "  channel: the number of channels in the input image\n",
        "  f1: the size of filter (must be odd #)\n",
        "  n1: the number of filter apply on layer 1\n",
        "  n2: the number of filter apply on layer 2\n",
        "  f3: the size of filter (must be odd #)\n",
        "  \n",
        "  \n",
        "  from: https://arxiv.org/abs/1501.00092\n",
        "  '''  \n",
        "  \n",
        "  if not isinstance(img_size, (int)):\n",
        "    raise Exception('img_size is not a valid size in srcnn model')\n",
        "  \n",
        "  initializer = RandomNormal(mean=0.0, stddev=0.001)\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(filters=n1, kernel_size=f1, padding='same', activation='relu',\n",
        "                   kernel_initializer=initializer, input_shape=(img_size,img_size,channel)))\n",
        "  model.add(Conv2D(filters=n2, kernel_size=1, padding='same', activation='relu',\n",
        "                  kernel_initializer=initializer))\n",
        "  model.add(Conv2D(filters=channel, kernel_size=f3, padding='same', activation='linear',\n",
        "                  kernel_initializer=initializer))\n",
        "  \n",
        "  # either SGD or Adam\n",
        "  #optimizer = SGD(lr=0.0003)\n",
        "  optimizer = Adam(lr=0.0003)\n",
        "  model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "  \n",
        "  model.summary()\n",
        "  \n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2cgYjtF81t1s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ]
    },
    {
      "metadata": {
        "id": "SGkYBCzi2AWC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model Config"
      ]
    },
    {
      "metadata": {
        "id": "h9UOu_Bm1uPg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b4b1f2ab-aef5-4277-92bd-8376ea5822fa"
      },
      "cell_type": "code",
      "source": [
        "# size of sub image\n",
        "size = 32\n",
        "\n",
        "# strde when crop image\n",
        "stride = 14\n",
        "\n",
        "# upscaling factor\n",
        "scale = 1\n",
        "\n",
        "# batch size\n",
        "batch = 32\n",
        "\n",
        "# size of low resolution and high resolution image is the same?\n",
        "same_size = True\n",
        "\n",
        "# greyscale ON or OFF? if ON, img will only contain Y channel\n",
        "greyscale = True\n",
        "\n",
        "# whcih training dataset you want to use?\n",
        "train_dataset_dir = '91-image'\n",
        "\n",
        "# which validation dataset you want to use?\n",
        "val_dataset_dir = 'Set5'\n",
        "\n",
        "# which testing dataset you want to use?\n",
        "test_dataset_dir = 'Set14'\n",
        "\n",
        "# which model you want to train or test?\n",
        "model = srcnn(size, 1, 9, 64, 32, 5)\n",
        "# set the name of model below\n",
        "model_name = 'srcnn'\n",
        "\n",
        "# epochs\n",
        "epochs = 15000\n",
        "\n",
        "# steps_per_epoch\n",
        "steps_per_epoch = 30\n",
        "\n",
        "# validation_steps\n",
        "validation_steps = 3\n",
        "\n",
        "# resume last traning?\n",
        "resume = False\n",
        "    "
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 32, 32, 64)        5248      \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 32, 32, 32)        2080      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 32, 32, 1)         801       \n",
            "=================================================================\n",
            "Total params: 8,129\n",
            "Trainable params: 8,129\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p2KU8_I55mV_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Make Data"
      ]
    },
    {
      "metadata": {
        "id": "PXgxe40I4OWB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "     _______________________________________________\n",
        "    | YOU SHOULD NOT CHANGE ANY THING IN THIS CELL. |\n",
        "    |_______________________________________________|\n",
        "\"\"\"\n",
        "\n",
        "# load tranning dataset\n",
        "train_lr, train_hr = load_set(train_dataset_dir, size, stride, scale, same_size, greyscale)\n",
        "\n",
        "# make tf.data dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_lr, train_hr))\n",
        "train_dataset = train_dataset.batch(batch)\n",
        "train_dataset = train_dataset.repeat()\n",
        "\n",
        "# load validation dataset\n",
        "val_lr, val_hr = load_set(val_dataset_dir, size, stride, scale, same_size, greyscale)\n",
        "# make tf.data dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_lr, val_hr))\n",
        "val_dataset = val_dataset.batch(batch//10)\n",
        "val_dataset = val_dataset.repeat()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qEco354UZowl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3193
        },
        "outputId": "0d61fb8b-e0e0-43a2-e25d-b9390d7a8cf9"
      },
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "     _______________________________________________\n",
        "    | YOU SHOULD NOT CHANGE ANY THING IN THIS CELL. |\n",
        "    |_______________________________________________|\n",
        "\"\"\"\n",
        "train(model, model_name, train_dataset, val_dataset, epochs, steps_per_epoch, validation_steps, resume)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15000\n",
            "30/30 [==============================] - 1s 32ms/step - loss: 15058.5146 - mean_squared_error: 15058.5156 - val_loss: 1047.0585 - val_mean_squared_error: 1047.0585\n",
            "Epoch 2/15000\n",
            "30/30 [==============================] - 0s 11ms/step - loss: 2337.1493 - mean_squared_error: 2337.1494 - val_loss: 520.1921 - val_mean_squared_error: 520.1921\n",
            "Epoch 3/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 1938.3423 - mean_squared_error: 1938.3423 - val_loss: 645.4485 - val_mean_squared_error: 645.4485\n",
            "Epoch 4/15000\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 1423.1133 - mean_squared_error: 1423.1134 - val_loss: 409.3376 - val_mean_squared_error: 409.3376\n",
            "Epoch 5/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 1077.6829 - mean_squared_error: 1077.6830 - val_loss: 717.2977 - val_mean_squared_error: 717.2977\n",
            "Epoch 6/15000\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 1557.5723 - mean_squared_error: 1557.5724 - val_loss: 292.6825 - val_mean_squared_error: 292.6825\n",
            "Epoch 7/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 1227.3912 - mean_squared_error: 1227.3911 - val_loss: 367.5746 - val_mean_squared_error: 367.5746\n",
            "Epoch 8/15000\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 1325.7025 - mean_squared_error: 1325.7024 - val_loss: 205.3793 - val_mean_squared_error: 205.3793\n",
            "Epoch 9/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 1286.4877 - mean_squared_error: 1286.4879 - val_loss: 236.2814 - val_mean_squared_error: 236.2814\n",
            "Epoch 10/15000\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 1022.9881 - mean_squared_error: 1022.9882 - val_loss: 174.6989 - val_mean_squared_error: 174.6990\n",
            "Epoch 11/15000\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 1466.0404 - mean_squared_error: 1466.0403 - val_loss: 172.3282 - val_mean_squared_error: 172.3282\n",
            "Epoch 12/15000\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 952.5834 - mean_squared_error: 952.5834 - val_loss: 109.3126 - val_mean_squared_error: 109.3126\n",
            "Epoch 13/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 690.1110 - mean_squared_error: 690.1111 - val_loss: 122.6227 - val_mean_squared_error: 122.6227\n",
            "Epoch 14/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 853.1074 - mean_squared_error: 853.1074 - val_loss: 121.8451 - val_mean_squared_error: 121.8451\n",
            "Epoch 15/15000\n",
            "30/30 [==============================] - 0s 10ms/step - loss: 723.7021 - mean_squared_error: 723.7021 - val_loss: 108.1728 - val_mean_squared_error: 108.1728\n",
            "Epoch 16/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 573.4393 - mean_squared_error: 573.4393 - val_loss: 120.7395 - val_mean_squared_error: 120.7395\n",
            "Epoch 17/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 559.9956 - mean_squared_error: 559.9957 - val_loss: 181.7544 - val_mean_squared_error: 181.7545\n",
            "Epoch 18/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 465.7775 - mean_squared_error: 465.7775 - val_loss: 296.2673 - val_mean_squared_error: 296.2672\n",
            "Epoch 19/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 246.9424 - mean_squared_error: 246.9424 - val_loss: 256.5869 - val_mean_squared_error: 256.5869\n",
            "Epoch 20/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 440.3732 - mean_squared_error: 440.3732 - val_loss: 343.8297 - val_mean_squared_error: 343.8297\n",
            "Epoch 21/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 371.8882 - mean_squared_error: 371.8882 - val_loss: 267.2807 - val_mean_squared_error: 267.2806\n",
            "Epoch 22/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 665.4133 - mean_squared_error: 665.4132 - val_loss: 505.5799 - val_mean_squared_error: 505.5799\n",
            "Epoch 23/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 262.7150 - mean_squared_error: 262.7150 - val_loss: 309.0642 - val_mean_squared_error: 309.0642\n",
            "Epoch 24/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 434.2664 - mean_squared_error: 436.1208 - val_loss: 476.3415 - val_mean_squared_error: 476.3415\n",
            "Epoch 25/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 603.6301 - mean_squared_error: 603.6301 - val_loss: 278.2026 - val_mean_squared_error: 278.2026\n",
            "Epoch 26/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 629.2129 - mean_squared_error: 629.2129 - val_loss: 314.0237 - val_mean_squared_error: 314.0237\n",
            "Epoch 27/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 484.0473 - mean_squared_error: 484.0473 - val_loss: 323.6448 - val_mean_squared_error: 323.6447\n",
            "Epoch 28/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 286.4230 - mean_squared_error: 286.4231 - val_loss: 262.7157 - val_mean_squared_error: 262.7157\n",
            "Epoch 29/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 409.6319 - mean_squared_error: 409.6319 - val_loss: 227.4264 - val_mean_squared_error: 227.4264\n",
            "Epoch 30/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 362.1740 - mean_squared_error: 362.1740 - val_loss: 244.6831 - val_mean_squared_error: 244.6830\n",
            "Epoch 31/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 490.0749 - mean_squared_error: 490.0750 - val_loss: 234.9621 - val_mean_squared_error: 234.9621\n",
            "Epoch 32/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 303.5942 - mean_squared_error: 303.5942 - val_loss: 240.2968 - val_mean_squared_error: 240.2968\n",
            "Epoch 33/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 356.1478 - mean_squared_error: 356.1478 - val_loss: 221.6440 - val_mean_squared_error: 221.6440\n",
            "Epoch 34/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 686.2973 - mean_squared_error: 686.2972 - val_loss: 186.8504 - val_mean_squared_error: 186.8504\n",
            "Epoch 35/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 563.3667 - mean_squared_error: 563.3668 - val_loss: 196.2297 - val_mean_squared_error: 196.2297\n",
            "Epoch 36/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 269.1406 - mean_squared_error: 269.1406 - val_loss: 116.0138 - val_mean_squared_error: 116.0138\n",
            "Epoch 37/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 615.9027 - mean_squared_error: 615.9026 - val_loss: 153.7603 - val_mean_squared_error: 153.7603\n",
            "Epoch 38/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 401.9282 - mean_squared_error: 401.9282 - val_loss: 121.7972 - val_mean_squared_error: 121.7972\n",
            "Epoch 39/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 429.4114 - mean_squared_error: 429.4114 - val_loss: 151.8360 - val_mean_squared_error: 151.8360\n",
            "Epoch 40/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 328.9164 - mean_squared_error: 328.9164 - val_loss: 160.8364 - val_mean_squared_error: 160.8365\n",
            "Epoch 41/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 358.6614 - mean_squared_error: 358.6613 - val_loss: 1039.2034 - val_mean_squared_error: 1039.2035\n",
            "Epoch 42/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 127.4696 - mean_squared_error: 127.4696 - val_loss: 145.3779 - val_mean_squared_error: 145.3779\n",
            "Epoch 43/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 216.7502 - mean_squared_error: 216.7503 - val_loss: 994.9532 - val_mean_squared_error: 994.9532\n",
            "Epoch 44/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 172.4673 - mean_squared_error: 172.4673 - val_loss: 174.6494 - val_mean_squared_error: 174.6494\n",
            "Epoch 45/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 470.1422 - mean_squared_error: 470.1422 - val_loss: 869.9893 - val_mean_squared_error: 869.9893\n",
            "Epoch 46/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 140.0734 - mean_squared_error: 140.0734 - val_loss: 298.3031 - val_mean_squared_error: 298.3031\n",
            "Epoch 47/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 195.2971 - mean_squared_error: 196.3360 - val_loss: 795.2263 - val_mean_squared_error: 795.2263\n",
            "Epoch 48/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 403.8299 - mean_squared_error: 403.8299 - val_loss: 445.6692 - val_mean_squared_error: 445.6692\n",
            "Epoch 49/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 281.7479 - mean_squared_error: 281.7479 - val_loss: 644.4819 - val_mean_squared_error: 644.4819\n",
            "Epoch 50/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 467.8256 - mean_squared_error: 467.8256 - val_loss: 554.4635 - val_mean_squared_error: 554.4634\n",
            "Epoch 51/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 181.5402 - mean_squared_error: 181.5403 - val_loss: 502.2962 - val_mean_squared_error: 502.2962\n",
            "Epoch 52/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 193.4237 - mean_squared_error: 193.4238 - val_loss: 626.2007 - val_mean_squared_error: 626.2007\n",
            "Epoch 53/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 234.7785 - mean_squared_error: 234.7785 - val_loss: 448.2527 - val_mean_squared_error: 448.2528\n",
            "Epoch 54/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 412.4689 - mean_squared_error: 412.4688 - val_loss: 631.4603 - val_mean_squared_error: 631.4603\n",
            "Epoch 55/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 147.4447 - mean_squared_error: 147.4447 - val_loss: 524.8127 - val_mean_squared_error: 524.8127\n",
            "Epoch 56/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 223.1615 - mean_squared_error: 223.1615 - val_loss: 629.8090 - val_mean_squared_error: 629.8090\n",
            "Epoch 57/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 551.5239 - mean_squared_error: 551.5240 - val_loss: 569.2959 - val_mean_squared_error: 569.2959\n",
            "Epoch 58/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 450.4396 - mean_squared_error: 450.4396 - val_loss: 607.2170 - val_mean_squared_error: 607.2170\n",
            "Epoch 59/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 199.7743 - mean_squared_error: 199.7742 - val_loss: 648.6557 - val_mean_squared_error: 648.6557\n",
            "Epoch 60/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 491.6197 - mean_squared_error: 491.6198 - val_loss: 494.0004 - val_mean_squared_error: 494.0004\n",
            "Epoch 61/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 307.5085 - mean_squared_error: 307.5085 - val_loss: 623.1530 - val_mean_squared_error: 623.1530\n",
            "Epoch 62/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 366.3776 - mean_squared_error: 366.3776 - val_loss: 503.0527 - val_mean_squared_error: 503.0527\n",
            "Epoch 63/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 250.8609 - mean_squared_error: 250.8609 - val_loss: 661.3861 - val_mean_squared_error: 661.3860\n",
            "Epoch 64/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 372.5765 - mean_squared_error: 372.5764 - val_loss: 483.5420 - val_mean_squared_error: 483.5419\n",
            "Epoch 65/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 106.7261 - mean_squared_error: 106.7261 - val_loss: 666.6347 - val_mean_squared_error: 666.6346\n",
            "Epoch 66/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 152.8414 - mean_squared_error: 152.8414 - val_loss: 569.3531 - val_mean_squared_error: 569.3531\n",
            "Epoch 67/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 120.1790 - mean_squared_error: 120.1790 - val_loss: 806.2798 - val_mean_squared_error: 806.2799\n",
            "Epoch 68/15000\n",
            "30/30 [==============================] - 0s 9ms/step - loss: 335.1873 - mean_squared_error: 335.1873 - val_loss: 606.4913 - val_mean_squared_error: 606.4913\n",
            "Epoch 69/15000\n",
            "30/30 [==============================] - 0s 8ms/step - loss: 168.8989 - mean_squared_error: 168.8988 - val_loss: 852.1408 - val_mean_squared_error: 852.1409\n",
            "Epoch 70/15000\n",
            " 8/30 [=======>......................] - ETA: 0s - loss: 56.0879 - mean_squared_error: 56.0879"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-24a64f3952a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m|\u001b[0m\u001b[0m_______________________________________________\u001b[0m\u001b[0;34m|\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresume\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-36-2c7d564b8777>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, model_name, train_set, val_set, epochs, steps_per_epoch, validation_steps, resume)\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   history = model.fit(train_set, epochs=epochs, callbacks=callbacks, steps_per_epoch=steps_per_epoch,\n\u001b[0;32m---> 33\u001b[0;31m             validation_data=val_set, validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0;31m# plot metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m           \u001b[0;31m# `ins` can be callable in DistributionStrategy + eager case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m           \u001b[0mactual_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mins\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m           logging.warning('Your dataset iterator ran out of data; '\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}